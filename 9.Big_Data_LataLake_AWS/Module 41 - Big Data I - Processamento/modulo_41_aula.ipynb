{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "modulo_41_aula.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJqp9AANOCtf"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/andre-marcos-perez/ebac-course-utils/main/media/logo/newebac_logo_black_half.png\" alt=\"ebac-logo\">\n",
        "\n",
        "---\n",
        "\n",
        "# **Módulo** | Big Data I - Processamento\n",
        "Caderno de **Aula**<br> \n",
        "Professor [André Perez](https://www.linkedin.com/in/andremarcosperez/)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9jDtUbDOE1-"
      },
      "source": [
        "# **Tópicos**\n",
        "\n",
        "<ol type=\"1\">\n",
        "  <li>Introdução;</li>\n",
        "  <li>Apache Spark;</li>\n",
        "  <li>Data Wrangling com Spark.</li>\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmoHgt-lwkpD"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GABI6OW8OfQ2"
      },
      "source": [
        "# **Aulas**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1\\. Introdução"
      ],
      "metadata": {
        "id": "-Rig-cljwJdi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.1. Big data** "
      ],
      "metadata": {
        "id": "Ay4UkojwGskg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUonSt7kxZyj"
      },
      "source": [
        "*Big data* é um termo que geralmente representa um conjunto de dados muito grande, complexo e de difícil processamento. Apesar do apelo comercial, o termo pouco contribui para a definição de um problema com o volume de dados pois:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " - tamanho é relativo, um problema de processamento em um computador pode não ser para outro que contenha mais recursos (RAM, CPU, etc.);\n",
        " - tamanho não é o único desafio moderno no processamento de dados: a variedade do tipo dos dados e a velocidade com que são produzidos somam-se a essa complexidade;\n",
        " - etc."
      ],
      "metadata": {
        "id": "y9PqivD6_u78"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YhdREnYxZyj"
      },
      "source": [
        "### **1.2. 3vs: Volume, variedade e velocidade** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHNT6j-IxZyk"
      },
      "source": [
        "A criação da *internet* (1990s) e a sua democratização através da adoção em massa de computadores pessoais (2000s), *smartphones* (2010s) e dispositivos da *internet* das coisas (2010s), trouxe diversos novos desafios para o ecossistema de dados em três frentes: **volume**, **variedade** e **velocidade**."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " - **Volume**: os recursos de um sistema computacional não são suficientes para processar um determinado volume de dados em uma determinada janela de de tempo / tamanho dos dados representa frações da memória do computador (décimos, etc.). Via de regra, arquivos com mais de 100 MB de tamanho são problemáticos para serem processados em computadores tradicionais."
      ],
      "metadata": {
        "id": "UsO_C7glvGGF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " > **Exemplo**: um arquivo de texto (txt) de *log* de acesso de usuários a um *website* facilmente atinge 100 MB de tamanho em poucos dias. Com esse arquivo é possível responder perguntas de negócio como: Qual é o período do dia/semana/mês/ano com mais acessos? Qual o tempo médio da etapa de *login*? Qual a taxa de erro de acesso por dia/semana/mês?"
      ],
      "metadata": {
        "id": "6dhaMIBfwGhI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " - **Variedade**: as fontes de dados modernas armazenam e disponibilizam dados em diversos formatos. Somaram-se aos tradicionais bancos de dados relacionais (SQL) diferentes formatos de arquivos de arquivos (csv, json, txt, html, pdf, jpeg, png, etc.), bases de dados não relacionais (NoSQL ou dados semi/não estruturados), APIs (json), etc."
      ],
      "metadata": {
        "id": "ZANjG6oHxJVZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " > **Exemplo**: os sites dos tribunais de justiça dos estados publicam diariamente o andamento dos processos judiciais que tramitam na segunda instância em arquivos do tipo `pdf`. Como fazer para extrair e armazenar estes arquivos diariamente? Como extrair o número e o *status* do processo do documento?"
      ],
      "metadata": {
        "id": "Xfd4NWoGxJVc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " - **Velocidade**: processamento de dados em lote (*batch*) já não atende mais as necessidades do negócio. Dispositivos permanecem conectados as redes de computadores (*internet*, *internet* móvel, etc.) o tempo todo, logo, continuamente produzindo dados."
      ],
      "metadata": {
        "id": "ikOp0k9O64jA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " > **Exemplo**: um *e-commerce* registra os *clicks* de um usuário enquanto este navega pelo seu *website*. Com este dados e com o histórico do usuário, seria possível disponibilizar um cupom de desconto para que o usuário não deixe o *website* sem finalizar uma compra? Qual o melhor momento para enviar o cupom?"
      ],
      "metadata": {
        "id": "thcxtgw664jH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnJI1bszxZyk"
      },
      "source": [
        "### **1.2. Computação distribuída e paralela** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OGjIWy9xZyk"
      },
      "source": [
        "A estratégia para lidar com o aumento da demanda por recursos para processamento de dados sempre foi a de melhoria do *hardware* de **um mesmo  computador**: mais memória, mais velocidade de processamento, etc. Contudo, após os anos 2000s, a demanda cresceu em um ritmo muito mais acelerado se comparado a capacidade de melhoria de *hardware*. E dessa necessidade nasceu uma nova arquitetura de computadores e um novo paradigma de computação: *clusters* de computadores (**múltiplos computadores**) e computação distribuída e paralela, respectivamente."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " - Arquitetura"
      ],
      "metadata": {
        "id": "so8nVIRsobiF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Um *cluster* é um conjunto de computadores (mesmas configurações, idealmente mesmo *hardware*, etc.) conectados em uma rede privada. Um gerenciador de *cluster* (*cluster manager*) é uma aplicação que orquestra as atividades de armazenamento e processamento de dados distribuído e paralelo, abstraindo a complexidade para usuários e aplicações. Os gerenciadores de *cluster* mais utilizados são o [Apache Hadoop](https://hadoop.apache.org) e o [Kubernetes](https://kubernetes.io)."
      ],
      "metadata": {
        "id": "ID37vkQECLu6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Computadores de um *cluster* são conhecidos como nós."
      ],
      "metadata": {
        "id": "XK2lQv-ICP0B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " - Armazenamento"
      ],
      "metadata": {
        "id": "PKhTGukRoecY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dados são armazenados em arquivos (`csv`, `txt`, `parquet`, etc.) e são \"quebrados\" em blocos (128 MB geralmente), distribuídos e replicados (três vezes geralmente) nos nós. O gerenciador de *cluster* mantém um mapa da distribuição dos blocos."
      ],
      "metadata": {
        "id": "yAexqnvsL4-N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " - Processamento"
      ],
      "metadata": {
        "id": "11BSwM_fogdh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Existem algumas maneiras de se processador dados distribuídos. Uma das maneiras mais eficiente é enviar a operação de processamento (agregações como soma, por exemplo) para o nó em que o dado está armazenado, realizar o processamento localizado e coletar apenas os resultados."
      ],
      "metadata": {
        "id": "rg18G89pL68i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " > **Nota**: Operações de junção (joins) constumam ser caras (em termos de tempo de processamento e consumo de memória) pois blocos inteiros de dados devem trafegar pela rede de um nó para o outro."
      ],
      "metadata": {
        "id": "EpWQdsKZC-_a"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvJTE2FpaGeU"
      },
      "source": [
        "## 2\\. Apache Spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISYXZbomlb1x"
      },
      "source": [
        "[Spark](https://spark.apache.org) é um motor de processamento de dados para engenharia, análise e ciência de dados otimizado para *clusters* de computadores. Permite que operações comuns no processamento de dados como seleção de colunas, filtros e *joins* sejam feitas de utilizando o paradigma de computação distribuída e paralela através de APIs de alto nível disponível em diversas linguagens.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<img src=\"https://spark.apache.org/docs/latest/img/cluster-overview.png\" alt=\"spark-cluster\">"
      ],
      "metadata": {
        "id": "Ggz79pBx93je"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O Spark divide as operações em **transformações** (`filter`, `select`, `withColumn`, etc.) e **ações** (`read.csv`, `write.csv`, etc.). Operações de **transformação** são encadeadas até que uma operação de **ação** seja executada, fazendo com que todas as operações sejam executadas de uma vez. Essa característica é conhecida como *lazy evaluation*."
      ],
      "metadata": {
        "id": "hc2HFEvv8gYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Nota**: Em geral, a instalação (item 2.1) e configuração (item 2.2) de um *cluster* Spark é feito por especialistas, como uma pessoa engenheira de dados. É comum uma pessoa analista/cientista de dados começar a interagir com um *cluster* Spark a partir do item 2.3."
      ],
      "metadata": {
        "id": "fsugU0WBjghG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Nota**: O [AWS EMR](https://aws.amazon.com/pt/emr/) (*elastic map reduce*) fornece *clusters* com gerenciador Apache Hadoop e com o Apache Spark instalado. Preço computado sobre a hora dos nós, máquinas virtuais do [AWS EC2](https://aws.amazon.com/pt/ec2/)."
      ],
      "metadata": {
        "id": "D4DlkVZCkYBP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.1. Instalação**"
      ],
      "metadata": {
        "id": "EhKOYuGqLMSo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spark é uma aplicação desenvolvida na linguagem de programação [Scala](https://www.scala-lang.org), que funciona em uma máquina virtual [Java](https://www.java.com/) (JVM). Por isso, é necessário fazer o *download* da aplicação e instalar o Java em todas as máquinas (nós) do *cluster*."
      ],
      "metadata": {
        "id": "_MlzEchdu0cP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " - Download do Spark, versão 3.0.0."
      ],
      "metadata": {
        "id": "Wui-si_DfkcT"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8a_Yv59zg3gm"
      },
      "source": [
        "%%capture\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.0.0-bin-hadoop2.7.tgz && rm spark-3.0.0-bin-hadoop2.7.tgz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " - Download e instalação do Java, versão 8."
      ],
      "metadata": {
        "id": "pDzIx14Ufv0U"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXls3bfoglKW"
      },
      "source": [
        "%%capture\n",
        "!apt-get remove openjdk*\n",
        "!apt-get update --fix-missing\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mesmo sendo uma aplicação Scala, o Spark disponibiliza APIs de integração em diversas linguagens de programação. O pacote Python [PySpark](https://pypi.org/project/pyspark/) é a API para Python. Com ele, é possível interagir com o Spark como se fosse uma aplicação Python nativa. A API é similar ao pacote Pandas e sua documentação pode ser encontrada neste [link](https://spark.apache.org/docs/latest/api/python/)."
      ],
      "metadata": {
        "id": "2VlE66tTgBso"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Nota**: a versão do PySpark deve ser a mesma que a versão da aplicação Spark."
      ],
      "metadata": {
        "id": "0_QZG1l-jKHF"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oSYOwKljPf5"
      },
      "source": [
        "!pip install -q pyspark==3.0.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnxYnr6flb1y"
      },
      "source": [
        "### **2.2. Configuração** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NY8c3fEWlb1y"
      },
      "source": [
        "Na etapa de configuração, é necessário configurar as máquinas (nós) do *cluster* para que tanto a aplicação do Spark quanto a instalação do Java possam ser encontrados pelo PySpark e, consequentemente, pelo Python. Para isso, basta preencher as variáveis de ambiente `JAVA_HOME` e `SPARK_HOME` com o seus respectivos caminhos de instalação."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZpR7NwOh2EB"
      },
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop2.7\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Por fim, para conectar o PySpark (e o Python) ao Spark e ao Java, pode-se utilizar o pacote Python [FindSpark](https://pypi.org/project/findspark/)."
      ],
      "metadata": {
        "id": "dIygkGUzmqjl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q findspark==1.4.2"
      ],
      "metadata": {
        "id": "Cg1V2D4IlcDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O método `init()` injeta as variáveis de ambiente `JAVA_HOME` e `SPARK_HOME` no ambiente de execução Python, permitindo assim a correta conexão entre o pacote PySpark com aplicação Spark."
      ],
      "metadata": {
        "id": "SyFGPWZclc-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "9fUZWhtNls9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_7ULt6Wlb1z"
      },
      "source": [
        "### **2.3. Conexão** "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Com o *cluster* devidamente configurado, vamos criar uma aplicação Spark. O objeto `SparkSession` do pacote PySpark (e seu atributo `builder`) auxiliam na criação da aplicação:\n",
        "\n",
        " - `master`: endereço (local ou remoto) do *cluster*;\n",
        " - `appName`: nome da aplicação;\n",
        " - `getOrCreate`: método que de fato cria os recursos e instância a aplicação."
      ],
      "metadata": {
        "id": "GUgbxTWaPGV0"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TxljJ_cwBCy"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName(\"pyspark-notebook\").getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Com o objeto `SparkSession` devidamente instanciado, podemos começar a interagir com os dados utilizando os recursos do *cluster* através de uma estrutura de dados que já conhecemos: `DataFrames`:"
      ],
      "metadata": {
        "id": "KMKEZbMlSVGX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q \"https://raw.githubusercontent.com/cluster-apps-on-docker/spark-standalone-cluster-on-docker/master/build/workspace/data/uk-macroeconomic-data.csv\" -O \"uk-macroeconomic-data.csv\""
      ],
      "metadata": {
        "id": "MzzGeUmCcixf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = spark.read.csv(path=\"uk-macroeconomic-data.csv\", sep=\",\", header=True)"
      ],
      "metadata": {
        "id": "YjyNGZHnqKUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.show()"
      ],
      "metadata": {
        "id": "I-_bVdugS-T3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.printSchema()"
      ],
      "metadata": {
        "id": "El4HVFmxTUiA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVejInuPakz5"
      },
      "source": [
        "## 3\\. Data Wrangling"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos utilizar a API Python do Spark, o pacote PySpark, para limpar os dados da aula 2."
      ],
      "metadata": {
        "id": "POlvHFptrQFE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Nota**: Atente-se sempre a natureza distribuída das operações. "
      ],
      "metadata": {
        "id": "ILiYqGpzGxL3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3.1. Exploração**"
      ],
      "metadata": {
        "id": "qNz6f1-6mDB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.count()"
      ],
      "metadata": {
        "id": "wMDjNrzCrR-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.columns"
      ],
      "metadata": {
        "id": "Y5NuGUZmdkpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(data.columns)"
      ],
      "metadata": {
        "id": "GHO3JIuXFUt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3.2. Limpeza**"
      ],
      "metadata": {
        "id": "nGD63S7KreZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O método `select` seleciona colunas do `DataFrame`. Já o método `withColumnRenamed` renomeia colunas."
      ],
      "metadata": {
        "id": "_BWlK9ObSFA-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.select([\"Description\", \"Population (GB+NI)\", \"Unemployment rate\"])"
      ],
      "metadata": {
        "id": "kr0cfKzxraf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.\\\n",
        "  withColumnRenamed(\"Description\", 'year').\\\n",
        "  withColumnRenamed(\"Population (GB+NI)\", \"population\").\\\n",
        "  withColumnRenamed(\"Unemployment rate\", \"unemployment_rate\")"
      ],
      "metadata": {
        "id": "daszUPqcr0D7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.show(n=10)"
      ],
      "metadata": {
        "id": "ITbmLD5grioS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O método `filter` seleciona linhas do `DataFrame` baseado no conteúdo de uma coluna."
      ],
      "metadata": {
        "id": "qjflPLf3R5XP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_description = data.filter(data['year'] == 'Units')"
      ],
      "metadata": {
        "id": "Lc7l4RbCri-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_description.show(n=10)"
      ],
      "metadata": {
        "id": "lnViipaTrn1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3.3. Junção**"
      ],
      "metadata": {
        "id": "J5MaetaA5yN8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(data.count(), len(data.columns))"
      ],
      "metadata": {
        "id": "bmJ-9j-Z5JTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(data_description.count(), len(data_description.columns))"
      ],
      "metadata": {
        "id": "4-wev7wj5plc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O método `join` faz a junção distribuída de dois `DataFrames`. Já o método `broadcast` \"marca\" um `DataFrame` como \"pequeno\" e força o Spark a trafega-lo pela rede."
      ],
      "metadata": {
        "id": "UnnMzkV1Rd41"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import broadcast"
      ],
      "metadata": {
        "id": "umbjFEHU4706"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.join(other=broadcast(data_description), on=['year'], how='left_anti')"
      ],
      "metadata": {
        "id": "x2iVm1DHrqdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.show(n=10)"
      ],
      "metadata": {
        "id": "9Y57q9GYru-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O método `dropna` remove todas as linhas que apresentarem ao menos um valor nulo."
      ],
      "metadata": {
        "id": "Xjvd_8hXQ3ny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.dropna()"
      ],
      "metadata": {
        "id": "cSdhEE6UryO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.show(n=10)"
      ],
      "metadata": {
        "id": "yPCQdpijr1vE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O método `withColumn` ajuda a criar novas colunas."
      ],
      "metadata": {
        "id": "ZN4BeYx7QkEv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.withColumn('century', 1 + (data['year']/100).cast('int'))"
      ],
      "metadata": {
        "id": "Ul8y_Z3LL8p4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.select(['century', 'year']).groupBy('century').agg({'year': 'count'}).show()"
      ],
      "metadata": {
        "id": "O-d2iGfgO-z_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O método `collect` é uma ação que coleta os resultados dos nós e retorna para o Python."
      ],
      "metadata": {
        "id": "vdIqbFPqQphM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "timing = data.select(['century', 'year']).groupBy('century').agg({'year': 'count'}).collect()"
      ],
      "metadata": {
        "id": "8zHObZUZQDzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "timing"
      ],
      "metadata": {
        "id": "LvJQgROrQJO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "timing[0].asDict()"
      ],
      "metadata": {
        "id": "W9iI-G-6QNQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3.4. Escrita**"
      ],
      "metadata": {
        "id": "yAXG0okPr4LO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O método `write.csv` persiste o `DataFrame` em formato `csv`. Já o método `repartition` controla o número de partições da escrita."
      ],
      "metadata": {
        "id": "0Uw43266RAvj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.repartition('century').write.csv(path=\"uk-macroeconomic-data-clean\", sep=\",\", header=True, mode=\"overwrite\")"
      ],
      "metadata": {
        "id": "dXEY2Hbqr42f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}